{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10759165,"sourceType":"datasetVersion","datasetId":6673795}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:46:56.152936Z","iopub.execute_input":"2025-03-04T13:46:56.153210Z","iopub.status.idle":"2025-03-04T13:46:57.184524Z","shell.execute_reply.started":"2025-03-04T13:46:56.153181Z","shell.execute_reply":"2025-03-04T13:46:57.183711Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hackaton-god4/SampleSubmission.csv\n/kaggle/input/hackaton-god4/train.csv\n/kaggle/input/hackaton-god4/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:46:57.185384Z","iopub.execute_input":"2025-03-04T13:46:57.185830Z","iopub.status.idle":"2025-03-04T13:47:00.564819Z","shell.execute_reply.started":"2025-03-04T13:46:57.185798Z","shell.execute_reply":"2025-03-04T13:47:00.563967Z"}},"outputs":[{"name":"stdout","text":"Using GPU\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\ntrain_df=pd.read_csv(\"/kaggle/input/hackaton-god4/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:47:00.565620Z","iopub.execute_input":"2025-03-04T13:47:00.566012Z","iopub.status.idle":"2025-03-04T13:47:02.639218Z","shell.execute_reply.started":"2025-03-04T13:47:00.565989Z","shell.execute_reply":"2025-03-04T13:47:02.638552Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom tf_keras.optimizers import Adam\nfrom tf_keras.callbacks import ReduceLROnPlateau, EarlyStopping , LearningRateScheduler\nfrom tf_keras.layers import Dropout, Dense, Input\nfrom tf_keras.models import Model\nfrom tf_keras.losses import SparseCategoricalCrossentropy\nfrom transformers import AdamW\nimport tf_keras.preprocessing.text as kpt\nfrom tf_keras.preprocessing.text import Tokenizer\nimport random\n\n# Check if a GPU is available\nif tf.test.gpu_device_name():\n    print(f\"Using GPU: {tf.test.gpu_device_name()}\")\nelse:\n    print(\"No GPU found, using CPU.\")\n\n# Combine title and content for analysis\ntrain_df['content'] = train_df['content'].fillna('')\ntrain_df['text'] = train_df['title'] + \" \" + train_df['content']\n\n# Map string labels to integers\nlabel_encoder = LabelEncoder()\ntrain_df['target'] = label_encoder.fit_transform(train_df['target'])\n\n# Split the data into training and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the data\ntrain_encodings = tokenizer(train_df['text'].tolist(), padding='max_length', truncation=True, max_length=128)\nval_encodings = tokenizer(val_df['text'].tolist(), padding='max_length', truncation=True, max_length=128)\n\n# Data augmentation function\ndef augment_data(text):\n    aug_text = kpt.text_to_word_sequence(text)\n    # Simple augmentation: shuffle the words\n    random.shuffle(aug_text)\n    return ' '.join(aug_text)\n\n# Apply data augmentation\ntrain_df['aug_text'] = train_df['text'].apply(augment_data)\ntrain_encodings_aug = tokenizer(train_df['aug_text'].tolist(), padding='max_length', truncation=True, max_length=128)\n\n# Convert to TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': tf.constant(train_encodings['input_ids']),\n     'attention_mask': tf.constant(train_encodings['attention_mask'])},\n    tf.constant(train_df['target'])\n)).shuffle(100).batch(32)\n\ntrain_dataset_aug = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': tf.constant(train_encodings_aug['input_ids']),\n     'attention_mask': tf.constant(train_encodings_aug['attention_mask'])},\n    tf.constant(train_df['target'])\n)).shuffle(100).batch(32)\n\ntrain_dataset_combined = train_dataset.concatenate(train_dataset_aug)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': tf.constant(val_encodings['input_ids']),\n     'attention_mask': tf.constant(val_encodings['attention_mask'])},\n    tf.constant(val_df['target'])\n)).batch(32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:47:02.640793Z","iopub.execute_input":"2025-03-04T13:47:02.641039Z","iopub.status.idle":"2025-03-04T13:57:36.876486Z","shell.execute_reply.started":"2025-03-04T13:47:02.641018Z","shell.execute_reply":"2025-03-04T13:57:36.875737Z"}},"outputs":[{"name":"stdout","text":"Using GPU: /device:GPU:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255687cf34e342ab898ccf83f3bc86e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03fcc590085b4aa0a333cb04290ef39b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c524ee41e7f641869a1292d21437f399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8851fe15cd4f7aa004ac60238f956b"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# Load pre-trained BERT model\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n\n# Create a new model with a dropout layer\ninput_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\nbert_outputs = bert_model(input_ids, attention_mask=attention_mask)\ndropout = Dropout(0.5)(bert_outputs.pooler_output)\noutput = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\nmodel = Model(inputs=[input_ids, attention_mask], outputs=output)\n\n# Prepare optimizer and loss function\nloss = SparseCategoricalCrossentropy(from_logits=False)\noptimizer = Adam(learning_rate=0.000001)\n\n# Define callbacks\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0000001)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Function to adjust the learning rate\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\nlr_scheduler = LearningRateScheduler(scheduler)\n\n# Number of folds for cross-validation\nnum_folds = 5\n\n# Initialize KFold\nkf = KFold(n_splits=num_folds)\n\n# Prepare datasets\nX_input_ids = np.array(train_encodings['input_ids'])\nX_attention_mask = np.array(train_encodings['attention_mask'])\ny = np.array(train_df['target'])\n\n# Cross-validation\nfold_no = 1\nfor train_index, val_index in kf.split(X_input_ids):\n    print(f\"Training fold {fold_no}...\")\n\n    X_train_input_ids, X_val_input_ids = X_input_ids[train_index], X_input_ids[val_index]\n    X_train_attention_mask, X_val_attention_mask = X_attention_mask[train_index], X_attention_mask[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        {'input_ids': tf.constant(X_train_input_ids),\n         'attention_mask': tf.constant(X_train_attention_mask)},\n        tf.constant(y_train)\n    )).batch(32)\n    \n    val_dataset = tf.data.Dataset.from_tensor_slices((\n        {'input_ids': tf.constant(X_val_input_ids),\n         'attention_mask': tf.constant(X_val_attention_mask)},\n        tf.constant(y_val)\n    )).batch(32)\n\n    # Compile and train the model\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=[reduce_lr, early_stopping, lr_scheduler])\n    \n    # Evaluate the model\n    y_pred_prob = model.predict(val_dataset)\n    y_pred = y_pred_prob.argmax(axis=1)\n    accuracy = accuracy_score(y_val, y_pred)\n    print(f\"Fold {fold_no} Validation Accuracy: {accuracy:.4f}\")\n\n    fold_no += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:57:36.877718Z","iopub.execute_input":"2025-03-04T13:57:36.878155Z","iopub.status.idle":"2025-03-04T16:05:20.651527Z","shell.execute_reply.started":"2025-03-04T13:57:36.878121Z","shell.execute_reply":"2025-03-04T16:05:20.650617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40e9b9d0df947cd99d711457bbcce05"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Training fold 1...\nEpoch 1/10\n443/443 [==============================] - 263s 516ms/step - loss: 1.5714 - accuracy: 0.3069 - val_loss: 1.3556 - val_accuracy: 0.4723 - lr: 1.0000e-06\nEpoch 2/10\n443/443 [==============================] - 225s 507ms/step - loss: 1.3455 - accuracy: 0.4421 - val_loss: 1.1247 - val_accuracy: 0.5855 - lr: 1.0000e-06\nEpoch 3/10\n443/443 [==============================] - 225s 507ms/step - loss: 1.0654 - accuracy: 0.5963 - val_loss: 0.9115 - val_accuracy: 0.6803 - lr: 1.0000e-06\nEpoch 4/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.8985 - accuracy: 0.6800 - val_loss: 0.8292 - val_accuracy: 0.7125 - lr: 1.0000e-06\nEpoch 5/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.8188 - accuracy: 0.7106 - val_loss: 0.7900 - val_accuracy: 0.7221 - lr: 1.0000e-06\nEpoch 6/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.7658 - accuracy: 0.7272 - val_loss: 0.7693 - val_accuracy: 0.7263 - lr: 1.0000e-06\nEpoch 7/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.7428 - accuracy: 0.7391 - val_loss: 0.7518 - val_accuracy: 0.7348 - lr: 1.0000e-06\nEpoch 8/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.7205 - accuracy: 0.7466 - val_loss: 0.7462 - val_accuracy: 0.7387 - lr: 1.0000e-06\nEpoch 9/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.6932 - accuracy: 0.7549 - val_loss: 0.7330 - val_accuracy: 0.7427 - lr: 1.0000e-06\nEpoch 10/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.6762 - accuracy: 0.7616 - val_loss: 0.7317 - val_accuracy: 0.7418 - lr: 1.0000e-06\n111/111 [==============================] - 20s 162ms/step\nFold 1 Validation Accuracy: 0.7418\nTraining fold 2...\nEpoch 1/10\n443/443 [==============================] - 242s 513ms/step - loss: 0.6795 - accuracy: 0.7613 - val_loss: 0.6208 - val_accuracy: 0.7788 - lr: 1.0000e-06\nEpoch 2/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6642 - accuracy: 0.7655 - val_loss: 0.6222 - val_accuracy: 0.7788 - lr: 1.0000e-06\nEpoch 3/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.6503 - accuracy: 0.7690 - val_loss: 0.6222 - val_accuracy: 0.7824 - lr: 1.0000e-06\nEpoch 4/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.6392 - accuracy: 0.7748 - val_loss: 0.6201 - val_accuracy: 0.7824 - lr: 1.0000e-06\nEpoch 5/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6210 - accuracy: 0.7815 - val_loss: 0.6236 - val_accuracy: 0.7827 - lr: 1.0000e-06\nEpoch 6/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.6100 - accuracy: 0.7839 - val_loss: 0.6238 - val_accuracy: 0.7827 - lr: 1.0000e-06\nEpoch 7/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.5964 - accuracy: 0.7898 - val_loss: 0.6280 - val_accuracy: 0.7799 - lr: 2.0000e-07\n111/111 [==============================] - 21s 162ms/step\nFold 2 Validation Accuracy: 0.7824\nTraining fold 3...\nEpoch 1/10\n443/443 [==============================] - 243s 514ms/step - loss: 0.6332 - accuracy: 0.7767 - val_loss: 0.5530 - val_accuracy: 0.8019 - lr: 2.0000e-07\nEpoch 2/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.6344 - accuracy: 0.7774 - val_loss: 0.5531 - val_accuracy: 0.8033 - lr: 2.0000e-07\nEpoch 3/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.6330 - accuracy: 0.7780 - val_loss: 0.5525 - val_accuracy: 0.8016 - lr: 2.0000e-07\nEpoch 4/10\n443/443 [==============================] - 225s 507ms/step - loss: 0.6238 - accuracy: 0.7805 - val_loss: 0.5521 - val_accuracy: 0.8028 - lr: 2.0000e-07\nEpoch 5/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6227 - accuracy: 0.7790 - val_loss: 0.5528 - val_accuracy: 0.8033 - lr: 2.0000e-07\nEpoch 6/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6188 - accuracy: 0.7817 - val_loss: 0.5523 - val_accuracy: 0.8036 - lr: 2.0000e-07\nEpoch 7/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6199 - accuracy: 0.7813 - val_loss: 0.5528 - val_accuracy: 0.8036 - lr: 1.0000e-07\n111/111 [==============================] - 21s 163ms/step\nFold 3 Validation Accuracy: 0.8028\nTraining fold 4...\nEpoch 1/10\n443/443 [==============================] - 241s 513ms/step - loss: 0.6237 - accuracy: 0.7793 - val_loss: 0.5403 - val_accuracy: 0.8081 - lr: 1.0000e-07\nEpoch 2/10\n443/443 [==============================] - 224s 507ms/step - loss: 0.6245 - accuracy: 0.7791 - val_loss: 0.5402 - val_accuracy: 0.8078 - lr: 1.0000e-07\nEpoch 3/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6250 - accuracy: 0.7793 - val_loss: 0.5407 - val_accuracy: 0.8084 - lr: 1.0000e-07\nEpoch 4/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6217 - accuracy: 0.7805 - val_loss: 0.5407 - val_accuracy: 0.8064 - lr: 1.0000e-07\nEpoch 5/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6199 - accuracy: 0.7790 - val_loss: 0.5408 - val_accuracy: 0.8078 - lr: 1.0000e-07\n111/111 [==============================] - 20s 162ms/step\nFold 4 Validation Accuracy: 0.8078\nTraining fold 5...\nEpoch 1/10\n443/443 [==============================] - 242s 513ms/step - loss: 0.6303 - accuracy: 0.7765 - val_loss: 0.5007 - val_accuracy: 0.8228 - lr: 1.0000e-07\nEpoch 2/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6317 - accuracy: 0.7759 - val_loss: 0.5011 - val_accuracy: 0.8220 - lr: 1.0000e-07\nEpoch 3/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6301 - accuracy: 0.7780 - val_loss: 0.5016 - val_accuracy: 0.8228 - lr: 1.0000e-07\nEpoch 4/10\n443/443 [==============================] - 224s 506ms/step - loss: 0.6314 - accuracy: 0.7773 - val_loss: 0.5025 - val_accuracy: 0.8217 - lr: 1.0000e-07\n111/111 [==============================] - 20s 162ms/step\nFold 5 Validation Accuracy: 0.8228\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\ntest_df=pd.read_csv(\"/kaggle/input/hackaton-god4/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:05:20.652494Z","iopub.execute_input":"2025-03-04T16:05:20.652826Z","iopub.status.idle":"2025-03-04T16:05:20.931411Z","shell.execute_reply.started":"2025-03-04T16:05:20.652794Z","shell.execute_reply":"2025-03-04T16:05:20.930743Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Combine title and content for the test data\ntest_df['content'] = test_df['content'].fillna('')\ntest_df['text'] = test_df['title'] + \" \" + test_df['content']\n\n# Tokenize the test data\ntest_encodings = tokenizer(test_df['text'].tolist(), padding='max_length', truncation=True, max_length=128)\n\n# Convert to TensorFlow dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': tf.constant(test_encodings['input_ids']),\n     'attention_mask': tf.constant(test_encodings['attention_mask'])}\n)).batch(32)\n\n# Predict on the test data\ny_test_pred_prob = model.predict(test_dataset)\ny_test_pred = y_test_pred_prob.argmax(axis=1)\n\n# Map integer predictions back to string labels\ntest_df['predicted_target'] = label_encoder.inverse_transform(y_test_pred)\n\n# Display the first few predictions\nprint(test_df[['id', 'title', 'predicted_target']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:05:20.932190Z","iopub.execute_input":"2025-03-04T16:05:20.932437Z","iopub.status.idle":"2025-03-04T16:06:13.991754Z","shell.execute_reply.started":"2025-03-04T16:05:20.932414Z","shell.execute_reply":"2025-03-04T16:06:13.990848Z"}},"outputs":[{"name":"stdout","text":"77/77 [==============================] - 15s 162ms/step\n      id                                              title  \\\n0   3639                                          Tailgated   \n1  21493  I am a model and because of mean things my mot...   \n2  21215     On-and-off nothingness. I donâ€™t know any more.   \n3  13466                              Feeling really scared   \n4  14084                  Looking for people who understand   \n\n                 predicted_target  \n0                         anxiety  \n1  relationship-and-family-issues  \n2  relationship-and-family-issues  \n3                      depression  \n4                         anxiety  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"test_df.iloc[3].content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:12:09.017246Z","iopub.execute_input":"2025-03-04T16:12:09.017714Z","iopub.status.idle":"2025-03-04T16:12:09.022905Z","shell.execute_reply.started":"2025-03-04T16:12:09.017682Z","shell.execute_reply":"2025-03-04T16:12:09.022234Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"Hello I am new to all this online forums and stuff, but Im just putting this out there, I didnt think I was depressed, but now Im feeling like it cant be anything else.  I am always so angry when I do the slightest thing wrong, such as spill something, and over the weekend, everytime I did something like that I got so angry started yelling then just broke down in tears.  I also suffer bouts of what I can admit is anorexia, I am currently 54 kgs but whenever I look in the mirror, I just see a fat woman staring back, I try not to talk about my weight with my family, they get upset when I call myself fat, its not that I think Im fat so much, I just feel fat, whether I have a medical condition that makes me feel bloated and weighed down or what I dont know.  I always feel tired and unmotivated to do anything, and can have trouble sleeping too. I also have a very low sex drive which is really taking its toll on my relationship, my partner thinks that I dont have any feelings for him anymore, which isnt true, I just feel like I hate myself and my body\\n \\nAre all these symptoms related to depression or could it be other underlying medical conditions and if so what is the best way to help me recover, Im a little scared to go to a doctor and get medication I dont like to take too many drugs\\n \\nThank you for any help \\nDear thadeedz,\\nIt sounds like you have heaps of symptoms at the moment, they sound really horrible to try and manage yourself. It is really important to go to a GP and let him/her know just what is happening for you.\\nYou could be experiencing symptoms from a range of things and ti is best to get yourself checked out. Going to a dr doesn't mean you have to take medication. But first things first, it is really important to reach out and get some support to \\n1. work out what is happening for you then\\n2. make a plan with the GP about how best to get through these difficulties.\\nPlease let us know how you are going.\\nthe moderators\\nHello dear,\\nI agree with ModeratorA. Go to a doctor. You may not need medication at all. You may also qualify for counselling. It will help you work out what works for you to get and feel better. Our hormones do strange things to us sometimes. It will help to have a good diet and exercise, too. Good luck with everything. Talk to someone soon. X\""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import os\nimport joblib  # Import joblib for saving the label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nsave_directory = '/kaggle/working/saved_model'\nos.makedirs(save_directory, exist_ok=True)\n\n# Save the tokenizer\ntokenizer.save_pretrained(save_directory)\n\n# Save the Keras model\nmodel.save(os.path.join(save_directory, 'bert_model'))\n\n# Save the label encoder\nlabel_encoder_filename = os.path.join(save_directory, 'label_encoder.pkl')\njoblib.dump(label_encoder, label_encoder_filename)\n\nprint(\"Model, tokenizer, and label encoder saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:06:13.992652Z","iopub.execute_input":"2025-03-04T16:06:13.992916Z","iopub.status.idle":"2025-03-04T16:06:54.056894Z","shell.execute_reply.started":"2025-03-04T16:06:13.992895Z","shell.execute_reply":"2025-03-04T16:06:54.056018Z"}},"outputs":[{"name":"stdout","text":"Model, tokenizer, and label encoder saved successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import shutil\n\n# Define the directory to be zipped\ndirectory_to_zip = '/kaggle/working/saved_model'\n\n# Define the output zip file path\nzip_output_path = '/kaggle/working/saved_model.zip'\n\n# Zip the directory\nshutil.make_archive(directory_to_zip, 'zip', directory_to_zip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:06:54.057824Z","iopub.execute_input":"2025-03-04T16:06:54.058132Z","iopub.status.idle":"2025-03-04T16:07:54.522677Z","shell.execute_reply.started":"2025-03-04T16:06:54.058100Z","shell.execute_reply":"2025-03-04T16:07:54.521908Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/saved_model.zip'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'saved_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:07:54.523659Z","iopub.execute_input":"2025-03-04T16:07:54.523901Z","iopub.status.idle":"2025-03-04T16:07:54.528893Z","shell.execute_reply.started":"2025-03-04T16:07:54.523882Z","shell.execute_reply":"2025-03-04T16:07:54.528055Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/saved_model.zip","text/html":"<a href='saved_model.zip' target='_blank'>saved_model.zip</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}